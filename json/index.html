

[
{
"title": "Project Work 6 - Outlook and Conclusion",
"url": "https://mt2-erlangen.github.io/conclusion/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\n6. Outlook and Conclusion\nCongratulations, you have now built all the tools needed to use the final provided plugin called User_Interface. \nIt allows you to play around with the different kinds of image-processing-techniques, you learned to apply over the course of this project.\nTools similar to those you implemented could for example be applied to facilitate or automate the diagnosis for radiologists.\nThere is therefore great research interest in segmentation, edge detection, and various similar processing methods.\nIn your report you should:\n\nName at least two examples of current trends in image processing. Provide citations for each example and describe them briefly.\nFor your examples, are they already applied to clinical routine? If not, do you think they soon will be? Try to explain why or why not.\n\nIn the last part, summarize what you have implemented and explained in your project report. Review the shortcomings of your approaches and how they could be mitigated in the future, and conclude your report.\n\nSubmission\nOnce you are finished with your code and satisfied with the results, please compress the entire src folder into a zip file and upload it to StudOn. \nAfterwards, write the report and submit the PDF file as well. \nThe deadline for submission is August 08th at 23:55 via StudOn. \nWe recommend that you submit earlier versions of your project to avoid accidentally missing the deadline. Only the last version will be considered.\nThank you for your time and interest! We look forward to reading your reports and hope to see you again for future lectures, projects, or theses! \nBest regards\nYour MT2-Team\n"
}

{
"title": "Project Work 5 - Canny Edge",
"url": "https://mt2-erlangen.github.io/cannyedge/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\n5: Canny-Edge\nThe Canny-Edge algorithm is one of the more advanced algorithms to perform edge detection. Unlike primitive approaches, like those you implemented in Task 4, Canny's algorithm leads to clearly defined edges (only one pixel in width) and a significant reduction in false detections (meaning regions of sharp brightness-transition, which are not edges of interest). \nThis is achieved through the following process: \n\nBlurring the image (Gaussian Blur) to reduce noise\nDetermining the image-gradient using the Sobel-kernel (→ Task 4)\nDetermining the gradient-direction for each pixel\nPerforming Non-Maximum-Suppression along the gradient-direction\nPerforming Hysteresis-Thresholding to select edges of intrest \n\nThese steps will be explained further later on.\n\n5.1: Blurring and Gradient\nThe first part of this task can be implemented directly in the run-method of the Task_5_CannyEdgeDetection-class.\nTo do:\n\n\nConvert the input-image to a FloatProcessor and apply a gaussian blur.\n\n&#128221;  Note:  \nThe $\\sigma$-parameter is one of the values you can play around with later on to improve your results. Once you are done, you will be able to set this value via a user-dialog. For now, a good starting point would be the value 2\n\n\n\nCreate 3 new FloatProcessors to store the image-gradient and the derivatives. Use the methods you implemented in Task 4 to apply a Sobel-operator and to calculate the gradient.\n\n\n\n5.2 Determining Gradient-Directions\nTo calculate the direction of each pixel, you will now implement a new method called getDir.\nThe formula for calculating the gradient-direction at a given pixel is:\n  &Theta; = atan2(Gy , Gx)  \nwith Gy and Gx being the values of the respective y- and x-derivatives at the current position.\n\n&#128221;  Note:  \nThe gradient-direction provides information about the angle or direction of an edge within the image. At any given point the edge will be perpendicular to the gradient direction. This will become important when it comes to performing Non-Maximum-Suppression (NMS). \nAn example: \n\n white pixels &#8793; edge; &nbsp; arrow &#8793; gradient-direction  \n\nThe atan2-method used to determine the direction returns the angle $\\Theta$, that results from converting a cartesian coordinate (x,y) to radians (r,$\\Theta$). The angle theta is therefore returned in radians and you will need to convert it to degrees. \n\n\n      &#9888;  Important warning:  &#9888;\n      \n      The atan2-method expects coordinates in a standard cartesian coordinate-system  (x→ / y↑). Since you are working with images, the y-axis is defined differently  (x→ / y↓) and you will therefore need to call the method like this:  Math.atan2 (-y, x) \n\nThe getDir-method will determine the gradient-direction for each pixel and then round it to one of the following values: 0°, 45°, 90°, 135°. These stem from the fact that an image is a discrete set of pixels and therefore we can only differentiate between these directions. \n\nGradient-directions: 0°, 45°, 90°, 135°\nTo do:\n\n\nCreate a new method:\npublic ByteProcessor getDir (FloatProcessor X_Deriv, FloatProcessor Y_Deriv){}\n\n\n\nCreate a ByteProcessor to store the directions\n\n\nCreate an int-array:\nint[] angles = {0,45,90,135,180};\n\n(180° is equivalent to 0° but needs to be considered as a seperate case)\n\n\nIterate through the input-FloatProcessors and calculate the direction for each pixel (in degrees). Remember that the y-axis is inverted.\n\n\nSearch for the closest match in the angles-array and store the final direction in the output-ByteProcessor. \n\n&#128221;  Note: \nNegative values are simply &quot;mapped&quot; to the corresponding positive value (for example -45° ≙ 135° or -90° ≙ 90°). You can do this by simply checking if the value is negative and then adding 180°. If the closest match is 180° the direction is set to 0°\n\n\n\nReturn the final ByteProcessor\n\n\n\n5.3: Non-Maximum-Suppression\nDuring NMS the goal is to reduce edges to a single-pixel-line. This is achieved by searching for local intensity-maxima in the Gradient-Direction, so that edge-information is preserved, but the blurriness of primitive edge-detection tools is removed.\nMore specifically, this works by checking each pixel in relation to its two neighbouring pixels (along the gradient-direction). If the pixel is the highest of the three, it is kept as part of the edge. If not, it is discarded (set to 0). \nTo do:\n\n\nCreate a new method:\npublic FloatProcessor nonMaxSuppress(FloatProcessor Grad, ByteProcessor Dir) {}\n\n\n\nCreate a new FloatProcessor to store the resulting image\n\n\nIterate through the gradient-image. Check the direction for each pixel and then evaluate whether or not it is a local maximum in gradient-direction.\n\n\nIf it is a local maximum, store the value in the output-FloatProcessor\n\n\nReturn the final FloatProcessor\n\n\n\n5.4: Hysteresis Thresholding\nHysteresis Thresholding is a special form of thresholding, which uses two threshold-values instead of one (upper and lower). Similar to standard thresholding, if a pixel's value falls above the upper threshold, it is kept as part of the image. If however, the pixel's value falls below, or is equal to the upper threshold, but above the lower threshold, the pixel is only kept as part of the image, if it is directly connected to a pixel above the upper threshold. Any pixel equal to or below the lower threshold is disregarded. \nTo do: \n\n\nCreate a new method: \npublic ByteProcessor hysteresisThreshold (FloatProcessor In, int upper, int lower){}\n\n\n\nSince you are working with a FloatProcessor and the values a pixel can have are not the easiest to work with, you can instead convert your input-values to percentages of the maximum value within the image. To do so, simply add: \nfloat tHigh = ((float)In.getMax()*upper)/100f;\nfloat tLow = ((float)In.getMax()*lower)/100f;\n\nYou can then use tHigh and tLow as your threshold values, while being able to define them through low integer numbers. As a starting point you can for example use 15 as upper and 5 as lower. Feel free to experiment around with these. \n\n\nCreate an output-ByteProcessor to store the final image \n\n\nIterate through the input image and check the threshold condition for each pixel. Set pixels above the upper limit to white in the output image \n\n\nIn order to check, whether a pixel above the lower threshold is connected to an existing edge, you will need to iterate through the image again and check the connections repeatedly, because a pixel can become connected to the edge through any number of adjacent pixels. \nTo avoid mistakes here, the following code, as well as the included hasNeighbours()-method will be provided. You can simply add this code after you performed the first iteration through the image.\n\nboolean changed = true;\nwhile (changed) {\n   changed = false;\n      for (int x = 0; x &lt; In.getWidth(); x++) {\n         for (int y = 0; y &lt; In.getHeight(); y++) {\n            if (In.getPixelValue(x, y) &gt; tLow &amp;&amp; hasNeighbours(Out, x, y) &amp;&amp; Out.getPixel(x,y)==0) {\n                     Out.set(x, y, 255);\n                     changed = true;\n                 }\n             }\n         }\n     }\n\n(Out refers to the output-image. If you named it differently, you can obviously change the code accordingly)\n\n\nReturn the output image\n\n\n\nAdd a simple user-dialog to the run-method, which allows you to select values for $\\sigma$, the upper threshold and the lower threshold.\nFinally perform the getDir,nonMaxSuppress and hysteresisThreshold steps in sequence within your run-method and display your final result.\n\n5.5: Project-Report\nThe part of your report concerning Task_5 should contain the following:\n\nA short description of what Canny-Edge-Detection aims to do and how it works\nIn which ways it is superior to the more primitive approaches\nImages you generated with your code. How do the parameters influence your results? \n\nNext\n"
}

{
"title": "Project Work 4 - Edge Detection",
"url": "https://mt2-erlangen.github.io/edgedetection/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\n4: Primitive Edge-Detection Filters\nSimilar to Thresholding and Segmentation, Edge-Detection is a commonly used technique in image processing (i.e. to descern boundarys of objects within an image etc.). In your project you will first be implementing a set of primitive edge-detection filters, as well as the more advanced Canny-Edge-Filter (Task 5). \n\n4.1: The Filter-Kernels\nThere are a variety of different Kernels used for edge detection; some of the most common ones are Sobel, Scharr, and Prewitt - Kernels.\n\nWhen applying these Filter-Kernels to an image through convolution, you essentially create the derivative of the image. \nThis is because these Kernels result in higher pixel-values in regions, where the image contains a sharp change in brightness (similar to derivatives in analysis). This &quot;derivation&quot; is performed in X- and Y-direction seperately.\nUsing both the X- and Y-derivative of an image, you can then generate the  image-gradient by calculating the euclidean norm over both derivatives at each pixel of the image. \n\n$G$ = $\\sqrt{G_{x} ^ 2+G_{y} ^ 2}$\n\nThis image-gradient will then show the edges as bright and the rest of the image as black.\n\n4.2: Filtering and Gradient\nTo do:\n\n\nOpen the Task_4_Filters-class and create a new method: \npublic FloatProcessor applyFilter (FloatProcessor In, int[][] kernel){}\n\n\n\nCreate a new FloatProcessor to store the resulting image\n\n\nIterate through the input image and perform the convolution \n\n&#128221;  Note:  \nSince you are working with a 3x3 kernel, you can't simply iterate through the entire image because you would encounter OutOfBounds-exceptions when getting to the rim of the image. \nFor the sake of simplicity you can therefore ignore the outermost row/column of pixels.\n\t\n\n\nReturn the resulting image\n\n\n\nNow that your plugin can perform a convolution (and therefore a derivation), you can calculate the image-gradient.\nTo do: \n\n\nCreate a new method:\npublic FloatProcessor getGradient (FloatProcessor In_X, FloatProcessor In_Y){}\n\n(In_X and In_Y are the derivatives in X- and Y-direction respectively)\n\n\nCheck if the input-images have the same dimensions, if not throw a fitting exception\n\n\nCreate a new FloatProcessor to store the resulting image\n\n\nIterate through the image and calculate the Gradient value for each pixel in the output-image\n\n\nReturn the resulting image-gradient\n\n\n\n4.3: User-Dialog\nAt this point your plugin contains everything needed to perform primitive edge-detection. \nAs a final step you will implement a simple user-dialog, which will allow the user to select between the three filters mentioned above. \nThe following code should be implemented in the run-method\nTo do:\n\n\nCreate a new GenericDialog\n\n\nCreate a String-array:\nString[] Filters = {&quot;Sobel&quot;,&quot;Scharr&quot;,&quot;Prewitt&quot;};\n\n\n\nAdd a popup-menu to select which filter you want to use \n\n&#128161;  Tip:  Check the ImageJ-API to see how popup-menus are implemented \n\n\n\nShow the dialog\n\n\nCheck if the dialog was cancelled. If it was, terminate the plugin \n\n\nGet the index (in the popup-menu) of the selected filter\n\n\nPerform the edge-detection using the selected filter and the methods you implemented\n\n\nShow your result\n\n\n\n4.4: Project-Report\nThe part of your report concerning Task_4 should contain the following:\n\nA short description on how these primitive Edge-Detection-Filters work\nPossible limitiations, which would require a more sophisticated approach to edge-detection\nImages you generated with your code\n\nNext\n"
}

{
"title": "Project Work 3 - Otsu",
"url": "https://mt2-erlangen.github.io/otsu/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\n3: Otsu's Method\nOtsu's method is a commonly used algorithm to compute the ideal threshold-value for image-segmentation. It is used in cases where the image-histogram is bimodal (meaning it contains two distinct peaks) to find the ideal &quot;middle ground&quot;. \nWe highly recommmend that you have a look at the original publication from 1975 regarding the algorithm (Access should be granted if you try to access it using the university internet).\n\n3.1: Theory\nOtsu's method works by maximizing the between class variance σB² which is defined as:\n\n&sigma;B2 (&theta;) = P1(&theta;) \t&middot; P2(&theta;) &middot; (&mu;1(&theta;) - &mu;2(&theta;))2 \nwith\nP1(θ) = $\\sum_{i = 0}^{\\theta}  h(i)$  (≙ number of pixels below the threshold (background))\nP2(θ) = 1 - P1(θ) = $\\sum_{i = \\theta +1}^{L-1}  h(i)$ (≙ number of pixels above the threshold (foreground))\nμ1(θ) = $\\frac{1}{P1(\\theta)}$ $\\cdot$ $\\sum_{i = 0}^{\\theta} (i+1)h(i)$  (≙ mean intensity of the background)\nμ2(θ) = $\\frac{1}{P2(\\theta)}$ $\\cdot$ $\\sum_{i = \\theta +1}^{L-1} (i+1)h(i)$  (≙ mean intensity of the foreground)\n\nwith h(i) being the normalized histogram of the image, θ being the current threshold and L being the length of the histogram-array.\n\n3.2: Coding\nIn order to implement this algorithm, you will need to:\n\nGenerate the histogram of the image\nUse the histogram to determine P1(θ) and P2(θ) for all possible θ's\nUse these values to calculate μ1(θ) and μ2(θ) for all possible θ's\nCalculate σB² (θ) for all possible θ's\n\nMoving foreward, these steps will be explained in further detail. \nSince your code for this task can get rather long, you should pay attention to an orderly programming style to avoid difficulties while debugging later on. You can also add comments to your code to help you keep track of your work. \nTo do:\n\nOpen the  Task_3_Otsu-class and take note of the empty methods provided. Each of these methods will be performing one of the calculations detailed above. \n\n  \n\n\nComplete the method:\n   public double[] getHistogram(ImageProcessor in) {}\n\na. Create a double-array of appropriate size to store the histogram-values\nb. Iterate through the input-image and update the corresponding histogram-entry for each pixel's value\nc. Normalize and return the histogram. \n\n\n\n   &#128221;  Note:  \n   Normalizing refers to converting the histogram to a probability distribution. If you are unsure how to do that, have a look at the original publication \n\n\n\n\nComplete the methods to compute P1(θ), P2(θ), μ1(θ) and μ2(θ):\npublic double[] getP1(double[] histogram){}\npublic double[] getP2(double[] P1){}\npublic double[] getMu1(double[] histogram, double[] P1){}\npublic double[] getMu2(double[] histogram, double[] P2){}\n\nP1(θ) and P2(θ):\n\n\nConsider which values for θ are possible in an 8-bit grayscale image\n\n\nIterate through the possible values of θ and calculate P1(θ) and P2(θ) for each instance \n\n\n\nμ1(θ) and μ2(θ):\n\nCalculate the values for μ1(θ) and μ2(θ) according to the formulas provided above. \n\n\n&#128221;  Note:  \nPay attention to the possibility of dividing by zero.\nYou can handle this, by checking beforehand, if you will be dividing by zero and simply dividing by a very small number instead. We recommend you use 10e-10\n\n\n\n  \n\nDetermine the values for σB² (θ) in the method:   public double[] getSigmas(double[] P1, double[] P2, double[] mu1, double[] mu2) {}\n\na. Create a new double-array of suitable length\nb. Calculate σB² (θ) for each value of θ and store it in the array you just created\nc. Return the array of sigmas\n\n  \n\n\nFind the maximum of your sigmas-array using:\n   public int getMaximum(double[] sigmas){}\n\nDetermine the index (within the array of possible σ's) of the maximum value for σB² (θ) and store the index as an int-variable\n(In case there is no definite maximum, you can simply select the σ with the highest index, as this adds the least amount of extra programming)\nThe maximum value this method returns is your Otsu-Threshold-Value\n\n\n\n\n\nComplete the method: \npublic ByteProcessor otsuSegmentation(ImageProcessor ip) {}\n\nThis method will combine all the steps for calculating the Otsu-Threshold, as well as return the Image after having applied the Otsu-Threshold and print the determined value to the terminal. \n\na. First apply an illumnation-correction to the input image. To do this, inherit the methods you implemented in Task_1 by using:\nTask_1_Threshold Threshold = new Task_1_Threshold();\n\nYou can call methods belonging to the Threshold-Object like this: Threshold.correctIllumination().\nb. Use the &quot;illumination-corrected&quot; image to perform all of the calculations you implemented \nc. Apply a Thresholding-operation to your image using the determined Otsu-Threshold and store the result in a ByteProcessor. \nd. Print the Otsu-Threshold to the terminal and return the result-ByteProcessor\n\n\n\nComplete the run-method such that it applies the Otsu-Segmentation-Process to the Input-Image and displays the resulting image.\n\n\nTo check your code you can perform an Otsu-Segmentation of the &quot;Cells&quot;-image.\nYour plugin should return the following: \n\n\n3.3: Project-Report\nThe part of your report concerning Task_3 should contain the following:\n\nA brief explanation of what Otsu's method is\nWhat it aims to achieve and how \nIts limitations\nExample of your segmentation\nIn the original publication, Otsu mentions that &quot;An optimal threshold is selected automatically and stably, not based on the differentiation (i.e. a local property such as valley), but on the integration (i.e., a global\nproperty) of the histogram.&quot; Explain what this means, especially where the integration comes from. \nCompare the results to the naive thresholding method \nOtsu's method, while still applied and useful in practice, has several shortcomings. Discuss two of them and name examples of current methods that can be applied to similar problems that solve these issues, by providing a citation and briefly explaining them. \n\nNext\n"
}

{
"title": "Project Work 2 - Segmentation",
"url": "https://mt2-erlangen.github.io/segmentation/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\n2: Segmentation\nFor your second task, you will be implementing a plugin, which will be capable of evaluating the quality of a segmentation by comparing it to a given reference-image. In clinical practice these reference images can be attained through manual segmentation of the image by a skilled physician.\nIn your case the provided reference image will look like this:\n\n\n2.1: A new class\nIn order to quantify the quality of your segmentation, you will be calculating the values for Sensitivity and Specificity, which were introduced in your blackboard-exercise. In addition, you will be creating a dedicated class to store and access these values. \nTo do:\n\nCreate a new class in the src-Folder named EvaluationResult \nCreate the class-variables sensitivity and specificity (both double)\nImplement a constructor:public EvaluationResult ( double specificity , double sensitivity ){}\n\n\nImplement getter-methods for both values: public double getSpecificity (){}\n public double getSensitivity (){}\n\n\n\n\n2.2: The Plugin\nThe rest of the code for this task will be implemented in the provided class Task_2_EvaluateSegmentation.\nIn order to calculate the values for Sensitivity and Specificity, you will need to assign a &quot;state&quot; to each pixel of your segmented image. The states in question are:\nState\nTP&quot;True Positive&quot; → the pixel was correctly identified as part of the target\nTN&quot;True Negative&quot; → the pixel was correctly identified  as background\nFP&quot;False Positive&quot; → the pixel was falsely identified as part of the target\nFN&quot;False Negative&quot; → the pixel was falsely identified as background\n\nFor decerning which of these cases applies to a given pixel, the reference image is used as the &quot;correct&quot; segmentation.\nThe values for Sensitivity and Specificity can then be calculated as follows: \n\n Sensitivity = TP/(TP+FN)  \n Specificity = TN/(TN+FP)   \n\nTo do:\n\nCreate a new method: private EvaluationResult evaluateSegmentation ( ImageProcessor\n segmentation , ImageProcessor reference ){}\n\n\nCheck if both images have the same dimensions - if not return null\nIterate over both images and count up the number of occurrances for each state \nCalculate the values for Sensitivity and Specificity using the formulas listed above\nCreate a new EvaluationResult-object to store these values and return it\n\n\nNow that you have created a method which perfroms the actual evaluation, you will need to implement the run-method in order to apply the plugin to an image. \nTo do:\n\n\nUse the IJ.openImage()-method to open a window, which allows the user to select the reference image.\n\n\nCheck wether the reference image has been loaded successfully - if not throw a fitting exception\n\n&#128161;  Tip:  \nCheck the ImageJ-API to see how the method behaves when no image has been loaded\n\n\n\nOnce the image was loaded successfully, apply your evaluateSegmentation()-method and print your results\n\n&#128221;  Note:   \nAs a test for your code, you could for example choose the cells_reference image as both &quot;segmentation&quot; and &quot;reference&quot;. The plugin should then return 1.0 for both values.\n\n\n\n\n2.3: Project-Report\nThe part of your report concerning Task 3 should contain the following:\n\nA brief explanation as to what TP, TN, FP, FN mean\nThe formulas for Specificity and Sensitivity\nA brief explanation on what Specificity and Sensitivity mean in the context of image-segmentation\n\nAdditionally you should also include your own results from this exercise:\n\nUsing your plugin from Task 1 generate 6 different segmentations of the &quot;cells&quot;-image using approapriate hyperparamters. \nUse the plugin you implemented in this task to evaluate each segmentation \nDisplay your results in an appropriate way \nName two other ways to evaluate how good the segmentation is\n\nNext\n"
}

{
"title": "Project Work 1 - Thresholding",
"url": "https://mt2-erlangen.github.io/thresholding/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\n1: Thresholding and Illumination-Correction\n1.1: Introduction\nYour first task consists of implementing an ImageJ-Plugin capable of performing a classic thresholding operation, as well as correcting for uneven illumination within an image. \nTo get you started, you have been provided with an incomplete class called Task_1_Threshold. \n\n1.2: Thresholding\nThe concept of thresholding - as the name implies - is based on evaluating an image pixel by pixel and checking each time, whether it falls above or below a given threshold-value.\nIf the pixel value in question is above the specified value, it will be set to white. If not it will be set to black. \nTo do:\n\n\nCreate a new method:\npublic ByteProcessor threshold ( ImageProcessor ip , int threshold ){} \n\n\n\nCreate a new ByteProcessor to store your result\n\n\nIterate over the entire input image and check the threshold-condition for each pixel\n\n\nSet each pixel in the output - ByteProcessor according to your evaluation\n\n\nReturn your result\n\n\n\n   &#128221;  Note:  \n   Do not use inbuilt methods provided by ImageJ to perform the thresholding operation \n      \n\n1.3: Illumination-Correction\nFor cases where the illumination of the image is uneven, you will now add the functionality to perform Illumination-Correction.\nTo do:\n\n\nCreate a new method:\npublic ByteProcessor correctIllumination ( ImageProcessor ip ){}\n\n\n\nConvert the input image to a FloatProcessor (make sure, that the original image remains unchanged)\n\n\nApply a Gaussian Filter to the newly created image ($\\sigma$ = 75) by using the blurGaussian()-function provided by ImageJ.\n\n\nDivide the original image by the filtered image (result should also be a  FloatProcessor)\n\n\nConvert your result to a ByteProcessor and return it\n\n\n\n   &#128161;  Tip:  \nThe division of two images can simply be performed by iterating over them and performing it \"pixelwise\". \nThere is however a method provided by ImageJ, which allows you to move (copy) the entire image-data of one image to another, while applying a simple operation (such as division) to every pixel of both images in one go. Check the ImageJ-API in case you want to use this method.\n\n\nTo allow you to test your results, the run-method already contains code for a simple user interface, which gives you the option to select a value for your threshold, as well as let you choose whether or not you want to correct the illumination before thresholding. This code has been commented out to avoid causing errors by calling methods you had not implemented yet.\nIt pays to have a look at this code now, as you will eventually be creating a few dialogues of your own in later tasks.\nConsider using the &quot;Cells&quot; image to try out your code, since it will be the image you will be working with for the rest of your project. \n\n1.4: Project Introduction\nTo begin the written section of your Final Project, you will first need to come up with an introduction. This introduction should:\n\nCapture the readers interest with a short motivation of the project \nSummarize and contextualize the approach to other research methods\nPosition your approach with respect to other approaches\nDefine the research problem and problem statement\nGive an overview of the paper's structure\n\n1.5: Project-Report\nThe part of your report concerning Task_1 should contain the following:\n\nA brief description of the methods you implemented\nProvide a mathematically sound formulation of the thresholding operation\nA short discussion on why it may be necessary to correct the illumination in microscopy-images\nImages you generated with your code, that showcase what you described \nCreate a figure that shows a few examples of the image before and after thresholding. What does correctIllumination do? \n\nNext\n"
}

{
"title": "Project Work 0 - Introduction",
"url": "https://mt2-erlangen.github.io/introduction/",
"body": "Overview\n\nIntroduction\nThresholding\nSegmentation\nOtsu's Method\nEdge Detection \nCanny Edge \nOutlook and Conclusion\n\nDisclaimer\nThis is a short introduction with general information and best practices, as well as guidelines for your project-report. We highly suggest you read through the entirety of this Introduction in order to get a detailed impression of the project-work ahead of you. We have also prepared a video with all the necessary information about the report. \nYou have been provided with a basic java template containing a fully set up project, as well as the empty task-classes you will be working with.\nFor information on the initial Setup, have a look at our getting started guide.\nFurthermore, you have been provided a latex-template as a starting point for your report. \nIt already defines the structure your report should have, so please do not change the order of the report-sections. \nIn cases where the images to expect after completing a task were given in the exercise, you should obviously only use images you generated with your own code in your report\nIf you are working on CIP machines, you may run into quota issues. You can fix these issues with this short guide.\nPlease also note, that you can connect remotely to CIP machines using a remote SSH connection\nIntroduction:\nThis project aims to introduce you to some fundamental image-processing-techniques which find application in a large variety of fields. \nSince you are studying to become biomedical engineers, you will mainly be working with an image of cells, as a simple example of images you might encounter over the course of your scientific career.\n\nThe techniques you will be working with are the following: \n\nBasic Thresholding / image-segmentation\nEvaluation of your segmentation\nOtsu-Segmentation\nPrimitive edge-detection\nCanny edge-detection\n\nBy the end of this project, you will have built a set of ImageJ-Plugins that are capable of performing all of these operations. In addition to that, you will be documenting your progress and results in the form of a written Project-Report. \nThis is why at the end of every task you will find a number of bullet-points detailing which topics you should include in your report. \nThe coding section of your project will be written in Java using IntelliJ as the editor of choice. You should already be familiar with coding in IntelliJ from lectures like AuD-MT as well as from your MT2 computer-exercises.\nYour written project report will be written in LaTeX using the online editor Overleaf. \nGeneral Information\nThe project report, as well as the coding, are individual work. As such, you need to submit them individually. Also, do not use any built-in methods that we do not specifically allow.\nNote: we'll check for plagiarism.\nReport Guidelines\nThe project report can be written in either English or German. Please write between 4 and 7 pages of text, not counting the images.\nWe expect you to:\n\n\nUse the LaTeX template we provide\n\nLaTeX template link\nDo not modify the style or the formatting. No ornaments for page numbers!\nThe template defines the overall structure of your project. You have to fill in all the gaps.\nDo not change the order of the sections in our template.\nDo not change the titles of sections or subsections.\nDo not change the order of the figures in the project. You can optionally add new figures to the report.\nWe will only count answers that appear in the correct subsection of the report. If you want to avoid repeating yourself, use \\label{} and \\ref{}.\nThe template contains examples for all commands necessary for the report. It is allowed to import and use other packages if desired. \n\n\n\nUse scientific references in your explanations to clearly separate your work from the work of others:\n\nUse the bibliography (see template Bib/bibliography.tex) and keep the citation style provided in the template.\nThe bibliography must be sorted (either alphabetically when using the Name/Year citation style or\nby the order you use them in the text when numbering the sources).\nDo not use more than two references that are websites only. \n\n\n\nAll symbols in equations need to be explained in the text.\n\n\nAll equations, figures and tables, if applicable, have to be numbered and referenced in the text.\n\nAll your figures should look professional.\nThey should not be blurry or hand-drawn and images should not have a window border of ImageJ.\nThey should not overlap with the text.\nAll figures need to have captions giving a brief description what the figure shows. The caption should be below the figure.\nLabel all axes in all plots and coordinate systems!\nA list of figures is not needed.\nReplace the images in the report template with images from your own implementation applied to knee data.\n\n\n\nDo not use abbreviations without introducing them. E.g., the first time you should write &quot;Magnetic Resonance Imaging (MRI)&quot;.\nAfter that, &quot;MRI&quot; is enough.\n\n\nJust like in storytelling, connect the context of the project report, so everyone can see the flow.\n\n\nDo not use footnotes!\n\n\nCheck your spelling: there shouldn't be any obvious spelling errors that can be detected by a spell checker.\n\n\nTo obtain all the points for the content of your report, additional to the above\n\nCheck whether you have addressed all the questions in the task description.\nCheck whether you have provided all the result figures and a detailed explanation of them.\n\nGuidelines for the Use of Writing Assistants\nWe welcome students to use writing assistants to enhance the quality of the written report. However, we would like to point out that\nstudents are responsible for the correctness of the content, and that scientific references are mandatory to verify all the claims made in the report.\nIf you decide to use any writing assistant, we ask you to add the tool to the list of references.\nThe use of spell-checking and translation software is encouraged and can be done without adding them to the list of references.\nNext\n"
}
,

{
"title": "Project Work 6 – Iterative Reconstruction and Conclusion",
"url": "https://mt2-erlangen.github.io/archive/2020/reconstruction/",
"body": "Iterative Reconstruction\nUsing backprojection, we could achieve a blurry reconstruction result.\nThe Filtered Backprojection algorithm solves this problem by applying a filtering step before backprojection.\nFor the project work, we will take a different approach.\nIn the last section, you measured the error between your reconstruction and the ground truth volume.\nHowever, this is only possible when doing a simulation and not when reconstructing an unknown real object.\nWhat we can do instead is meassuring the error in the projection domain by simply projecting the reconstruction!\nImplement the following method to use with our reconstructionProjector:\n    // In mt/Projector.java\n    public void reconstructIteratively(Image meassuredProjection, int sliceIdx, int numIterations)\n\nIt should\n\ncall projectSlice on volume to obtain a projection of our reconstruction\ncalculate an error image subtracting singogram.getSlice(sliceIdx) from meassuredProjection\nreplace the current slice of singogram by our error image\ncall backprojectSlice with the current sliceIdx\nrepeat all this for numIterations iterations\n\n\nSo we're now doing an reconstruction of the error sinogram and adding it to our blurry image.\nDoes this reduce our error?\nOur reconstruction algorithm is now finished. But it operates only on 2-d slices.\nCreate 3-d versions of projectSlice, backprojectSlice and reconstructIteratively:\n    public void project()\n    public void backproject()\n    public void reconstructIteratively(Volume measuredProjections, int numIterations)\n\nAll they should do is calling their 2-d version for each slice.\nYou should now be able to reconstruct volumes.\nHint: You can use the following construct instead of a for-loop to enable multi-threaded calculation.\n    // You have to replace `var` by `java.util.concurrent.atomic.AtomicInteger` when using Java 1.8\n    var progress = new java.util.concurrent.atomic.AtomicInteger(0);\n    IntStream.range(0, sinogram.depth()).parallel().forEach(z -&gt; {\n        System.out.println(&quot;Progess: &quot; + (int) (progress.incrementAndGet() * 100.0 / (double) sinogram.depth()) + &quot; %&quot;);\n        //Do stuff here for slice z\n        ...\n    });\n\nProject Report\nFor the project, describe how your iterative reconstruction algorithm works. You should not mention implementation details\nlike variable or function names. Compare it with the Filtered Backprojection algorithm! It's not necessary to explain \nFiltered Backprojection Algorithm in detail. Just highlight the main difference.\nTest your reconstruction algorithm on a slice of a CT reconstruction of the Cancer Imaging Archive.\nMeasure the error of the reconstructed slices after each iteration (so call reconstructIteratively with numIterations == 1).\nInclude a figure showing this error in dependence of the iteration number in the project report.\nInclude images comparing ground truth, the backprojected slice and the result after a few iterations.\nComment on the error and the images in your text.\nDoes the result of the iterative reconstruction look better than solely using backprojection?\nThis part of the project report should be no longer than 1.5 pages.\nConclusion\nIn the last part, summarize want you have implemented and explained in your project report.\nReview the shortcommings of your simplified approach and how they could be mitigated in future.\nDraw a conclusion on your work!\nThis part of the project work should be about a quarter page long and should contain no images.\nSubmission\nSubmit your project report as a PDF and your entire project folder of your code until August 16 23:55h.\nYour project must compile as a whole!\nMake sure that you had a last look at our checklist.\nEvaluation\nWe hope you had a fun project work!\nYou can help us to improve the instructions for next year!\nPrevious section\n"
},

{
"title": "Project Work 5 – Backprojection",
"url": "https://mt2-erlangen.github.io/archive/2020/backprojection/",
"body": "Backprojection\nIf we have a look at the sinogram values corresponding to one detector position we get some information about the projected object.\nFor instance, we can see the profile of the projected circle in the following image.\n\nHowever, if we have no access to the original volume slice we can not tell anything about the distance of the object to the detector.\nAll the following situations would generate the same projection!\n\nSo apparently, we get some information in the direction of the detector plane, but all information orthogonal to the detector plane\nis lost.\nSo one thing that we can do if we want to perform a reconstruction from the sinogram is to take the information in direction of the detector plane\nand uniormly smear it into the direction orthogonal to the detector plane in a range where we assume the object is located.\nWe call this process backprojection.\n\n\n    \n\n\n    The backprojection smears the value of the projection uniformly over the paths of the rays\n\n\nUse the following method, that is calculating the value that we want to smear back.\n    // in mt.Projector\n    public float backprojectRay(mt.Image sinogramSlice, int angleIdx, float s) {\n        sinogramSlice.setOrigin(0.f, -sinogram.physicalHeight * 0.5f);\n        return sinogramSlice.interpolatedAt(angleIdx * sinogram.spacing, s) // * sinogram.spacing is necessary because spacing is not valid for our angle indices (actually each coordinate should have their own spacing. That&#39;s the revenge for us being lazy.).\n                / (volume.physicalWidth() * Math.sqrt(2)) // we guess that this is the size of our object, diagonal of our slice\n                / sinogramSlice.width()  // we will backproject for each angle. We can take the mean of all angle position that we have here.\n                ;\n    }\n\nUse this method in backprojectSlice to backproject for each pixel x, y a horizontal line of the sinogram (all possible angles).\n    // in mt.Projector\n    public void backprojectSlice(int sliceIdx)\n    // A helper method\n    public void backprojectSlice(int sliceIdx, int angleIdx)\n\nTo do this \n\nCreate a loop over all angleIdx\n\nCall the helper method for all angle indices (there are sinogram.width angles)\n\n\nIn public void backprojectSlice(int sliceIdx, int angleIdx)\n\nGet the slice with index sliceIdx\nLoop over all x, y of this image\nCalculate the physical coordinates from the integers x and y (times spacing plus origin!)\nCalculate the actual angle theta from the angleIdx\nCalculate s from the physical coordinate.\n\ns is the physical distance of the point $\\vec{x}$ from the ray through the origin at angle theta.\nCan you write down the line equation for this line?\nCan you use the line equation to calculate the distance of $\\vec{x}$ an the line through the origin?\n\n\nCall backprojectRay with angleIdx and s\nAdd this result of backprojectRay to current value at position x, y and save the sum at that position\n\n\n\n\n\n\nReconstruction\nNext, we want to try out whether we can use our backprojection to reconstruct a volume.\nWhenever we want to test whether a method works, we need something to compare it with.\nThe best possible result, the &quot;true&quot; values, is usally called ground truth.\nWe can use one of the reconstructions that we downloaded from the Cancer Imaging Archive as a ground truth volume.\nThe best possible result for our reconstruction is to come as close as possible to the original (ground truth) volume.\nCreate a file src/main/java/project/GroundTruthReconstruction.java.\n// Your name &lt;your idm&gt;\npackage project;\n\nimport mt.Projector;\nimport mt.Volume;\n\nclass GroundTruthReconstruction {\n\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n    }\n}\n\nIt's important that we never mix up the ground truth with the results of our algorithm.\nCreate therefore an instance of Projector that will have the task to simulate projections.\nYou can call it groundTruthProjector.\nOpen a test volume and create an empty (all pixels 0) sinogram. They are needed to call the constructor of Projector.\nCall groundTruthProjector.projectSlice with an arbiray slice index.\n\nCreate an empty volume (all pixels 0)  with the same dimensions as the ground truth volume and a copy of groundTruthProjector.sinogram().\nYou can add the following method to mt.Volume to create copies.\n    // in mt/Volume.java\n    public Volume clone(String name) {\n        Volume result = new Volume(width(), height(), depth(), name);\n        IntStream.range(0, depth()).forEach(z-&gt; result.getSlice(z).setBuffer(Arrays.copyOf(slices[z].buffer(), slices[z].buffer().length)));\n        return result;\n    }\n\nCreate a new projector reconstructionProjector with the empty volume and the copy of our sinogram.\nUse backprojectSlice(...) to create your first reconstruction of a slice.\nA good way to test your implementation is to incremently apply more and more backprojections on your reconstruction.\nWhen you calculated the sinogram for SLICE_IDX you can use\n// in project.GroundTruthReconstruction.java\n\n// Choose the slice in the middle. Hopefully showing something interesting.\nfinal int SLICE_IDX = ????; // &lt; Use a index for which you already calculated `projectSlice`\n\nfor (int i = 0; i&lt; projector.numAngles(); i++ ) {\n    try {\n        TimeUnit.MILLISECONDS.sleep(500);\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n    projector.backprojectSlice(SLICE_IDX, i);\n    projector.volume().getSlice(SLICE_IDX).show();\n\n    //// Optionally save the intermediate results to a file:\n    //DisplayUtils.saveImage(projector.volume().getSlice(SLICE_IDX), &quot;/media/dos/shepp_9_&quot;+i+&quot;.png&quot;);\n}\n\nThis will wait 500ms between each backprojection. Do your rays meet at the right points? Use a simple test image with\nonly a single white circle if not. This should help you debug the issue.\n\n    \n         \n            \n            \n             \n        \n         \n            \n            \n             \n        \n    \n    \n        Backprojection using 9 views\n        Backprojection using 100 views\n    \n\nProject Report\nFor the project report, you should briefly describe your backprojection reconstruction algorithm.\n\nDescribe your implementation, create at least one figure supporting your explanations.\nYou should never mention implementation details like for-loops or variable names, but important parameters like the number\nof projection angles you used\nTest your reconstruction algorithm\n\nusing a simple test image like a white circle or square\nusing a CT reconstruction that you downloaded . Cite the data source!\n\n\nHow do images look like? If they are blurry, what is the reason for that.\nShow the images in your project report.\nMention in one sentence how the Filtered Backprojection algorithm tries to solve that problem.\nHow big are your errors in comparison to the ground truth? If you are using a measure like the Mean Squared Error give\na formula defining it.\n\nThe content for this section should be about one page long. \nPrevious section\nNext section\n"
},

{
"title": "Project Work 4 – Sinogram",
"url": "https://mt2-erlangen.github.io/archive/2020/sinogram/",
"body": "Sinogram\n\nNow, you should be able to generate sinograms from volume slice.\nGenerate two sinograms from two volume slices:\n\n\nOne sinogram from a simple test image. You can use for instance a white circle as I was doing in the last section.\n\n\nOne sinogram from a real CT reconstruction. You should cite the source of that image. The Cancer Imaging Archive even\nexplains you how to do that.\n\n\nShow both the volume slices and the sinograms.\nExplain to the reader what they are seeing. What is the radon transform?\nCan the radon transform be inverted?\n\n\nDo the sinograms contain some kind of symmetry? What is the reason for that?\nDo we really need a 360° degree scan?\n\n\n\n\nThis section should not be longer than one page.\nPrevious section\nNext section\n"
},

{
"title": "Project Work 3 – Projection",
"url": "https://mt2-erlangen.github.io/archive/2020/projection/",
"body": "Projections\nTo understand how we can reconstruct a volume from X-ray images, we will first go through the process of how these X-ray images\nwere acquired from a physical volume.\nIn your project report you should...\n\nexplain the reader the physical process of X-ray attenuation and its material dependance.\nWhat materials in the human body attenuate more X-rays than others?\nHow is this represented in a CT reconstruction? Or in other words: what quantity does a CT reconstruction actually show?\nWhich kind of tissues appear therefore lighter and which darker?\nexplain the fundamental theorem hat describes this process (X-ray attenuation). Give a formula!\nExplain all the symbols that you use in the formula.\nprove your explanations with references, also provide the source of the formula.\n\nIn this project work, we will make some simplifying assumptions on the acquisition geometry.\nI made a drawing of the path of a single X-ray through a slice of our volume.\nSince this ray crosses the origin of our coordinate system we call it the principal ray.\n\nWhat are the coordinates $\\vec{x}_{P}$ of a point $P$ on the line of the principal ray in dependency of the angle $\\theta$ ($\\alpha$ in drawing) and the distance\nfrom origin $r$?\nIn reality, not all X-rays cross the coordinate origin. \nWhat are the coordinates $\\vec{x}_{P'}$ of a point $P'$ that is on a ray that hits the detector at coordinate $s$ in depedency of $r$ and $\\theta$?\nWe assume parallel rays.\nHint: What vector do you have to add to $P$ to get to $P'$?\n\nUnfortunally, the figure was written on paper and you shouldn't use hand drawn figures in the project report (as you can see they look ugly).\nPlease create one or two plots on the computer that are explaining your derived the ray equations to the reader of the project\nreport. Decide which information is important for the reader to understand your text.\n\nHow does the described situation differ from the actual acquisition geometry of modern CT scanners?\nWhat are the reasons for that? Could our simplified situation be implemented in reality?\n\n\n\nAfter Implementation: Describe briefly your implementation of the projection.\nDo not refer any Java classes or variable names!\nGive a formula for how you calculated the different projection angles.\nGive a formula for how you calculated the projection result for each ray.\nWhat physical effects were neglected in our simulation but are present in reality?\nName at least three non-idealities of real systems.\n\nThis part of the project work should be not longer than 1.5 pages.\nAfter some remarks from you: 2 pages are also ok..\nImplementation\nWe already have an volume class which can store the stack of image slices. Additionally, we also want\nto store the projection images (referred as sinograms) for these stack of image slices. For this create\ncreate a class mt.Projector in a file src/main/java/mt/Projector.java, which can hold both volume slices\nand the sinograms.\n// Your name here &lt;your idm&gt;\npackage mt;\n\nimport java.util.stream.IntStream;\n\npublic class Projector {\n    // Our volume\n    private mt.Volume volume;\n    // Our sinogram\n    private mt.Volume sinogram;\n\n}\n\nImlement a constructor for this class.\nIt should call this.volume.centerOrigin() and set the origin of each sinogram slice to 0.0f, -sinogram.physicalHeight() * 0.5f so we use the same coordinate\nsystems as in our drawings (it might be handy to set the origin of sinogram to 0.0f, -sinogram.physicalHeight() * 0.5f, -sinogram.physicalDepth() * 0.5f, requires a Volume.setOrigin method)\n    public Projector(mt.Volume projectionVolume, mt.Volume sinogram) {\n        ... // Implementation here\n        assert sinogram.depth() == volume.depth() : &quot;Should have same amount of slices&quot;;\n    }\n\nConstructor and Setters/Getters:\n    public void setSinogram(Volume sinogram)\n    public Volume sinogram()\n\n    public void setVolume(Volume volume)\n    public Volume volume()\n\n    public int numAngles() // == sinogram.width()\n\nWe assume that we aquire $N$ projections at $N$ different angles $\\theta$.\nAll angles should have the same distance from each other and divide $2\\cdot \\pi$ in $N$ equal parts (we always use radians for angles).\nImplement a method which computes angle value of $n^{th}$ angle index. We want to use the method such that at $n=0$ our angle value should return $\\theta=0$, at $n=1$ returns $\\theta= \\frac{2\\cdot \\pi}{N}$, and so on. Think of a general formula to compute the $n^{th}$ angle and describe it briefly in the description of your implmentation.\nUse this formula to implement the following method:\n    // In mt.Projector\n    public float getNthAngle(int angleIdx)\n\nNow, recall the formula you derived for the position of point $P'$ in the previous section.\nWe could directly use those coordinates $\\vec{x}$ to calculate the integral in Lambert-Beer's law for a ray with angle $\\theta$ and shift $s$ over a slice $\\mu$ on our computers:\n$$ I_{\\textrm{mono}} = I_{0} \\cdot  \\exp\\left(-\\intop\\mu\\left(\\vec{x}\\right)\\textrm{d}\\vec{x}\\right) = I_{0} \\cdot  \\exp\\left(-\\intop_{-R}^{R}\\mu\\left(r,\\theta, s\\right)\\textrm{d}r\\right)$$\n$R$ is the radius of the circle circumscribing our rectangular slice. You can see it in the drawing.\nThe path integral goes along the path marked in yellow in the drawings.\nWe are only interested in the value of the line integral\n$$ P(\\theta, s) = \\intop_{-R}^{R}\\mu\\left(r, s, \\theta\\right)\\textrm{d}r $$\nand we have to replace the integral by a sum (computers cannot calculate integrals directly)\n$$ P(\\theta, s) = \\sum_{r=-R}^{R}\\mu\\left(r,\\theta, s\\right) \\cdot \\mathtt{spacing}$$\nCalculate this sum for a fixed $s$ and $\\theta$ on a slice of our volume!\nYou can use volumeSlice.interpolatedAt(x,y) to deterime $\\mu(\\vec{x})$ and access values of our slice.\n    // in mt.Projector\n    public float projectRay(mt.Image volumeSlice, float s, float theta)\n\nWe have now calculated one value of one of the gray rays on our slice which translates to one point in our sinogram.\n\nNext we want to call this function for every ray and every pixel of our sinogram in the following method:\n    // in mt.Projector\n    public void projectSlice(int sliceIdx) {\n\nTo do that ...\n\n\nGet the slice sliceIdx from this.volume using getSlice\n\nThis is a slice of our volume with coordinates $x$ and $y$.\n$x$ runs from left to right\n$y$ runs from top to bottom\n\n\n\nGet the sinogram for that slice sliceIdx from this.sinogramm using getSlice\n\nThis is a slice of our sinogram with physical coordinates $s$ and $\\theta$.\n$\\theta$ runs from left to right\n$s$ runs from top to bottom\n\n\n\nIterate over each pixel of the sinogram. I would use angleIdx, sIndex  as a loop variables.\n\nCalculate the actual value of s from sIndex.\nCalculate theata from angleIndex by calling the function getNthAngle\nCall projectRay with s and theta\nSave the result to sinogram at positions angleIndex and sIndex\n\n\n\nHint Computing s from sIndex is just using the physical coordinates and shifting the origin of $s$ axis in\nthe sinogram to the center.\nThis can be done by muliplying sIndex with sinogram.spacing() (pixel size of the detector) and adding\nsinogram.origin()[1] (== -sinogram.physicalHeight() * 0.5f).\nWe recommend you to test your algorithm using a simple image.\nChoose a good size for the sinogram to capture the whole image (e.g. height == volume.height).\nFor simplicity, you do not need to change the spacing of the volume or the sinogram.\n\n \n    \n    \n\n \n    Simple test slice\n    Sinogram of that slice\n\n\nI used a high number of 500 angles to get a near square image.\nWhen you are using less angles the width of your sinogram will be smaller.\nUse less angles to compute the results faster.\nYou may also apply projectSlice on all slices and display the sinogram.\nCtrl+Shift+H should reveal a rotating torso when using one the Cancer Archive scans:\n\n  \n \nOr of the test image above\n\n  \n \nPrevious section\nNext section\n"
},

{
"title": "Project Work 2 – Volumes",
"url": "https://mt2-erlangen.github.io/archive/2020/volume/",
"body": "Getting started\nImportant: You have to work alone on your project work. No team partners allowed anymore 😔!\nCT reconstruction treats the problem of recovering a three-dimensional volume from a set of X-ray images.\nSo we will need two classes that represent our volume and our stack of X-ray projections.\nIt turns out that we can interpret our projections and our volume just as a list of 2-d images.\n\n\n\n\n\nA volume: very much just multiple images stacked one over another\n\n\nCreate a class mt.Volume\n// Your name &lt;your idm&gt;\n// No team partner... So sad 😢!\n\npackage mt;\n\nimport java.util.Arrays;\nimport java.util.stream.IntStream;\n\npublic class Volume {\n    // Here we store our images\n    protected mt.Image[] slices;\n\n    // Dimensions of our volume\n    protected int width, height, depth;\n\n    // Spacing and origin like for mt.Image\n    protected float spacing = 1.f; // spacing is now our voxel size\n    protected float[] origin = new float[]{0, 0, 0}; // position of the top-left-bottom corner\n\n    // A name for the volume\n    protected String name;\n\n}\n\nCreate a constructor. Remember: width, height, depth, name must be set and slices must be created as an array.\nWe need depth images of size width $\\times$ height for the slices.\n    public Volume(int width, int height, int depth, String name)\n\nGetters/setters...\n    public int width()\n    public int height()\n    public int depth()\n    public float physicalWidth() // width * spacing()\n    public float physicalHeight() // height * spacing()\n    public float physicalDepth() // depth * spacing()\n\n    public mt.Image getSlice(int z) \n    public void setSlice(int z, mt.Image slice)\n\n    public float spacing()\n    public void setSpacing(float spacing) // should also set spacing also for all slices!\n    public String name()\n    public float[] origin()\n\n    // should set origin to (-0.5 physicalWidth, -0.5 physicalHeight, -0.5 physicalDepth) and call centerOrigin on each slice\n    public void centerOrigin()\n\nNow comes the interesting part: visualize the volume!\nYou will need to update src/main/java/lme/DisplayUtils.java file and use the following command to visualize the volume.\n    public void show() {\n        lme.DisplayUtils.showVolume(this);\n    }\n\nYou can download a volume from the Cancer Imaging Archive.\nUse one of the following links (it does not matter which CT volume you use).\n\nVolume 1\nVolume 2\nVolume 3\n\nUnzip the folder and drag the whole folder onto a running ImageJ, e.g. by the following code snippet in a file src/main/java/project/Playground.java.\n(if you have problems unzipping the files you might try the official downloader from the website. You need their downloader to open the *.tcia files).\n// This file is only for you to experiment. We will not correct it.\n\npackage project;\n\nimport mt.Volume;\n\nclass Playground {\n\n    public static void main(String[] args) {\n        // Starts ImageJ\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n        // You can now use drag &amp; drop to convert the downloaded folder into a *.tif file\n        \n    }\n\n}\n\n\nSave it the opened DICOM as a *.tif file (File &gt; Save As &gt; Tiff...).\nThere are more smaller test volumes on studOn.\n\n  \n \n\n\n\nOpen the saved tiff file in the main of a file src/main/java/project/Playground.java:\n// This file is only for you to experiment. We will not correct it.\n\npackage project;\n\nimport mt.Volume;\n\nclass Playground {\n\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n        \n        Volume groundTruth = DisplayUtils.openVolume(&quot;path/to/file.tif&quot;);\n        groundTruth.show();\n        \n    }\n\n}\n\n\nYou can now scroll through the different slices.\nvia GIPHY\nHere a short summary of handy functions of ImageJ when working with CT images.\n\nCtrl+Shift+C: Brightness and Contrast\nCtrl+Shift+H: Orthogonal Views (view volume from three sides)\nAfter selecting a line: Ctrl+K Line Plot\nCtrl+I: Get patient information of a DICOM\nLook at a 3-d rendering with ClearVolume\n\nPrevious: Introduction \nNext: Forward Projection\n"
},

{
"title": "Project Work 1 – Introduction",
"url": "https://mt2-erlangen.github.io/archive/2020/introduction/",
"body": "Contents\n\nIntroduction Tafelübung 9. Juni\nVolumes\nProjection Tafelübung 16. Juni\nSinogram\nBackprojection and Reconstruction Tafelübung 23. Juni\nIterative Reconstruction and Conclusion\n\nIntroduction\nDuring this semester we will learn how computer tomography (CT) reconstruction algorithms work.\nYour first task is to find out more about CT and write an introduction for your project report.\n\nFind an informative title for your project report. &quot;Project Report&quot; and &quot;Introduction&quot; are not good titles.\nWhat is computer tomography?\nWhat is the problem it tries to solve? When and how was it first introduced?\nWhat kind of electromagnetic radition is used to aquire the images?\nHow did modern CT devices improve over their predecessors? What is the typical spatial resolution of a state-of-the-art CT scanner?\nWhat are advantages and disadvantages of CT in comparison with other modalities. Include at least two advatages and\ntwo disadvantages.\nGive a short overview of the contents of the following sections of your project report.\nProof all your statements with references. You should use at least four distinct sources in your introduction that are\nnot webpages.\n\nThe introduction should not be longer than one page and but at least half a page. \nYour introduction and conclusion should not contain any images.\nPlease have a look on our checklist for a good project report.\n\n\nNext task\n"
},

{
"title": "Exercise 6",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-6/",
"body": "Submission deadline: 29.06.20 23:55h\nIn the last exercise, we want to have a look at edge detection and segmentation.\nEdge Detection\n 7 Points\nOpen a test image in a new file src/main/java/exercise/Exercise06.java.\n// Your name\n// Team parnter name\npackage exercises;\n\nimport lme.DisplayUtils;\nimport mt.LinearImageFilter;\n\npublic class Exercise06 {\n    public static void main(String[] args) {\n\t(new ij.ImageJ()).exitWhenQuitting(true);\n\tmt.Image cells = lme.DisplayUtils.openImageFromInternet(&quot;https://upload.wikimedia.org/wikipedia/commons/8/86/Emphysema_H_and_E.jpg&quot;, &quot;.jpg&quot;);\n\n    }\n}\n\nWe will use the Sobel Filter, to estimate the gradient of the image.\nThe Sobel Filter uses two filter kernels. One to estimate the x-component of the gradient and one for the y-component.\n\nCreate two LinearImageFilters with those coeffients. You can use filterX.setBuffer(new float[]{...})\nor setAtIndex to do that.\nFilter the original image with both of them!\n\n    \n\t\n\t\n    \n    \n\tX component of gradient $\\delta_x$\n\tY component of gradient $\\delta_y$\n    \n\nYou should now have two intermediate results that can be interpreted as the x-component $\\delta_x$\nand y-component $\\delta_y$of the estimated gradient for each pixel.\nUse those two images to calculate the norm of the gradient for each pixel!\n$$ \\left|\\left| \\nabla I \\right|\\right| =\\left|\\left| \\left(\\delta_x,\\ \\delta_x \\right) \\right|\\right| = \\sqrt{ \\delta_x^2 + \\delta_y^2}$$\n\nFind a good threshold and set all gradient magnitude values to zero that are below this values and all other to 1.f to\nobtain an image like this with a clear segmentation in edge pixels and non-edge pixels.\n\nSegmentation\n 3 Points\n\n Source: https://commons.wikimedia.org/wiki/File:Emphysema_H_and_E.jpg (cc-by-2.0)\nFor histologic examinations colored subtances called stains are used to enhance the constrast\nof different portions of the tissue.\nUse a suitable threshold to segment the individual sites with high contrast (0 background, 1 contrasted cells).\nYou can use the following method to overlay your segmentation with the original image.\n    // In lme.DisplayUtils\n    public static void showSegmentedCells(mt.Image original, mt.Image segmented) \n    // You may also try `showSegmentedCells(cells, segmentation, true);` with the newest version of DisplayUtils\n\n\nImproving your Segmentation\nThis is optional and not required for the exercise.\nYou might want to go directly to the evaluation of this year's exercises:\nhttps://forms.gle/2pbmuWtmeTtaVcKL7\nYou may notice that by just choosing a threshold you may not be able to separate each individual structure.\n\nYou can try out some operations from the menu Process &gt; Binary while you have your 0/1 segmentation focused.\nYou have to convert to 8-bit first. E.g.\n\n\n\nImage &gt; Type &gt; 8-bit\nProcess &gt; Binary &gt; Watershed\n\n\nOr &quot;click&quot; on menu items in your program code.\n    segmentation.show();\n    IJ.run(&quot;8-bit&quot;);\n    IJ.run(&quot;Watershed&quot;);\n    DisplayUtils.showSegmentedCells(cells, segmentation);\n\n\nEvaluation\nWe redesigned the exercises from scratch for this semester.\nTherefore, some of the exercises might have been difficult to understand or too much work. \nWe are glad for your feedback to help future semesters' students😊:\nhttps://forms.gle/2pbmuWtmeTtaVcKL7\n\n\n\n\n\n"
},

{
"title": "Exercise 5",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-5/",
"body": "Submission\nSubmission deadline: 08.06.20 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nQuanitfying Errors\n3 Points\nIn Exercise03, we have seen that we can use linear low-pass filters, like the Gauss filter, to reduce \nthe amount of noise in images. Let's test that!\nAdd two static methods to the Image class:\npublic static float meanSquaredError(Image a, Image b);\npublic static float psnr(Image a, Image b, float maxValue); // maxValue is 255 for PNG images\n\n\n$$ \\mathrm{MSE}_{ab}=  \\frac{1}{M} \\sum _{i=0}^{M} \\left(a_i - b_i\\right)^2 $$\n$$        \\mathrm{PSNR_{ab}} = 20\\cdot  \\log_{10}(\\mathtt{maxPossibleValue}) - 10\\cdot \\log_{10}(\\mathrm{MSE}_{ab}) $$\n\nStatic also means that you will use them like float mse = Image.meanSquaredError(imageA, imageB);.\nOpen a test image and add some noise using addNoise in exercise.Exercise05 (src/main/java/exercise/Exercise05).\n    (new ij.ImageJ()).exitWhenQuitting(true);\n    Image original = lme.DisplayUtils.openImageFromInternet(&quot;https://mt2-erlangen.github.io/shepp_logan.png&quot;, &quot;.png&quot;);\n    original.setName(&quot;Original&quot;);\n    \n    Image noise = new Image(original.width(), original.height(), &quot;Noise&quot;);\n    noise.addNoise(0.f, 10.f);\n\n    Image noisyImage = original.minus(noise); // You might also implement your own `plus` ;-)\n\nApply a Gauss filter (choose a good filterSize and sigma) on the noise image and compare the result with the original image.\nCan the error be reduced in comparision to the unfiltered noisy image? Also take a look on the error images that you can\ncalculate using your minus method of the class Image.\n\nHint: You can use a for-loop to try out different values for sigma.\nHint: You do not need to submit written answers to the questions in the text. Just do the correponding experiments!\n\nNon-Linear Filters\n3 Points\nA quality criterion for medical images are sharp edges.\nHowever, though the Gauss filter reduces the noise it also blurs out those edges.\nIn this exercise, we try to mitigate that problem using non-linear filters.\nNon-linear filters calculate similar to a convolution each pixel value in the output from a neighborhood of the\ninput image. Remember the sliding window from exercise 3? Non-linear filters do exactly the same.\n\nSource: https://github.com/vdumoulin/conv_arithmetic\nCreate a class mt.NonLinearFilter in the file src/main/java/mt/NonLinearFilter.java:\n// Your name here &lt;your idm&gt;\n// Your team partner here &lt;partner&#39;s idm&gt;\npackage mt;\n\nimport lme.WeightingFunction2d;\nimport lme.NeighborhoodReductionFunction;\n\npublic class NonLinearFilter implements ImageFilter {\n\n    // Name of the filter\n    protected String name; \n    // Size of the neighborhood, 3 would mean a 3x3 neighborhood\n    protected int filterSize;\n    // Calculates a weight for each neighbor\n    protected WeightingFunction2d weightingFunction = (centerValue,neighborValue,x,y) -&gt; 1.f;\n    // Calculates output value from neighbors and weights\n    protected lme.NeighborhoodReductionFunction reductionFunction;\n\n    public NonLinearFilter(String name, int filterSize) {\n        this.filterSize = filterSize;\n        this.name = name;\n    }\n\n    @Override\n    public String name() {\n        return name;\n    }\n}\n\nAs you can see, NonLinearFilter uses two interfaces. You can copy them into your src/main/java/lme/ folder.\n// in file `src/main/java/lme/WeightingFunction2d.java`\npackage lme;\n\n@FunctionalInterface // Does nothing. But Eclipse is happier when it&#39;s there.\npublic interface WeightingFunction2d {\n    // Assigns  a neighbor (shiftX, shiftY) a weight depending on its value and the value of the pixel in the middle of the neighborhood\n    float getWeight(float centerValue, float neighborValue, int shiftX, int shiftY);\n}\n\nand\n// in file `src/main/java/lme/NeighborhoodReductionFunction.java`\npackage lme;\n\n@FunctionalInterface\npublic interface NeighborhoodReductionFunction {\n    // Calculates the output pixels from the values of the neighborhood pixels and their weight\n    float reduce(float[] values, float[] weights);\n}\n\nImplement the method apply for NonLinearFilter.\n    @Override\n    public void apply(Image input, Image result)\n\nThe method should calculate each output pixel from a neighborhood. So\n\nCreate an array to hold the values of the neighborhood pixels. How many neighborhood pixels are there?\nLoop over each output pixel\n\nFill the array of neighborhood pixels with values from the input image (needs two inner loops)\nUse this.reductionFunction.reduce to determine the value of the output pixel. You can use null for the second parameter for now (we will implement weights later).\nSave the value to the output image (using setAtIndex).\n\n\n\nOverall, the method should look very similar to your LinearImageFilter.apply method.\nTo test your method, implement a MedianFilter in a file src/main/mt/MedianFilter.java as a subclass of NonLinearFilter.\n// Your name here\n// Team partner&#39;s name here\npackage mt;\n\nimport java.util.Arrays;\n\npublic class MedianFilter extends NonLinearFilter {\n\tpublic MedianFilter(int filterSize) {\n            // TODO:\n            super(...);\n            reductionFunction = ...;\n\t}\n}\n\nThe MedianFilter is a LinearImageFilter with\nreductionFunction (values, weights) -&gt; { Arrays.sort(values); return values[values.length / 2]; }\n(it sorts the values and takes the one in the middle).\nAll you need to do is to call the super constructor and set reductionFunction.\nDoes the median filter also reduce the noise in the image?\nBilateral Filter\n2 Points\nNext, we will implement the BilateralFilter.\npackage mt;\n\npublic class BilateralFilter extends NonLinearFilter {\n    GaussFilter2d gaussFilter;\n\n    public BilateralFilter(int filterSize, float spatialSigma, float valueSigma){\n        ...\n    }\n}\n\nThe bilateral assign a weight to each neightborhood pixel.\nSo modify your NonLinearFilter.apply method that it also creates a weights array and uses weightingFunction.getWeight to\nfill it. reductionFunction should now also be called with the weights array.\nThe bilateral has to parameters $\\sigma_{\\text{value}}$ and $\\sigma_{\\text{spatial}}$.\nFor large values of $\\sigma_{\\text{spatial}}$ the bilateral filter behaves like a Gauss filter.\nInitialize gaussFilter in the constructor. Set weightingFunction so that the weights $w_s$ of the Gauss filter are returned.\nSet reductionFunction. It should multiply each of the values with its weight and then sum the results up.\nYour BilateralFilter should now behave like a Gauss filter. Does it pass the test in GaussFilter2dTests when you\nuse BilateralFilter instead of GaussFilter2d?\nEdge-Preserving Filtering\n2 Points\nTo make our bilateral filter edge preserving, we have to use also $\\sigma_{\\text{value}}$.\nThe value weight $w_v$ is calculated as follows\n$$ w_v = \\exp\\left(-\\frac{\\left(\\mathtt{centerValue}-\\mathtt{value}\\right)^2}{2 \\sigma_{\\text{value}}^2}\\right) $$\nJust multiply with this value $w_v$ in weightingFunction. The total weight of a pixel will then be $w_v \\cdot w_s$.\nNow we have the problem that our weights will no longer add up to one! To solve this problem divide by the sum of weights\nin the reductionFunction.\nCan you reduce the error even more using the bilateral filter? My results look like this.\n\n  \n    \n    \n    \n    \n  \n  \n    Original\n    Noisy\n    Gauss filtered\n    Bilateral filtered\n  \n  \n    \n    \n    \n    \n  \n  \n    \n    Error Unfiltered\n    Error Gauss\n    Error Bilateral\n  \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
},

{
"title": "Exercise 4",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-4/",
"body": "Submission\nSubmission deadline: 01.06.20 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImage Transformations\nIn the previous exercises, we built a Signal and Image class for performing basic operations on the \ninput data. We also implemented various filters to process the data and remove noise. \nIn this exercise we will build on top of the image class and implement methods for performing image transformations.\nIn many medical applications there is a need to align two images so that we \ncan combine the information between the images. This can be due to the images coming from \ndifferent modalities like (CT and MRI) or in scenarios were you have an patient data at from \ndifferent time (before and after an surgery)  and you want to compare between these two images. \nIn all these scenarios we use image registration bring the different images together.\nIn the below image, two x-ray views (1) and (2) are fused together to obtain the combined view(3)\nwhich produces more information for diagnosis. This is achieved using image registration between view(1) and view\n \nImage Source: Hawkes, David J., et al. &quot;Registration and display of the combined bone scan and \nradiograph in the diagnosis and management of wrist injuries.&quot; European journal of nuclear medicine \n18.9 (1991): 752-756. \nOne of the crucial components of image registration is image transformations.\nIn this exercise we will implement basic image transformations. Additionally, we need to implement an \ninterpolation method to find out the image intensity values at the transformed coordinates. \n\nOverview of tasks\n\nWe will implement the following tasks for this exercise.\n\nHelper functions (a. Image origin, b. Interpolation)\nImage Transformation (a. Translation, b. Rotation, c. Scaling)\n\nWe introduce the basic theory about image transformations in theoretical background section.\nPlease read the theory before proceeding since we don't re-introduce everything in the task description. \nTask Description\n\nWe provide the main method for the task with an interactive ImageJ plug-in in the files\nsrc/main/java/exercises/Exercise04.java\nand src/main/java/mt/ImageTransformer.java\n\n0. Getting started\n1 Point\n\n\nFor Exercise 4 we provide a GUI that displays the image with different image transformation options.\n\n\n\nOnce you have all the transformations implemented you should be able to adjust the sliders and perform the desired transformations in an interactive manner.\n\n\nThe transformations requires an origin point about which we perform all the transformation.\n\n\nExtend the Image class with these three methods\n\n\n    // store the origin points x,y as \n    // a class variable\n    public void setOrigin(float x, float y)\n\n    // the origin() returns the {x,y} as float \n    // array from the stored origin class variable. \n    public float[] origin()\n\n    // Sets the origin to the center of the image\n    public void centerOrigin()\n\n\nTo ensure that everything is running, run the main function.\nWe already set the origin point for you in the file src/main/java/exercises/Exercise04.java\nTo ensure that everything is running, run the main function of Exercise04.\n\n1. Image interpolation\n4 Points\n\n\nSince the image transformations heavily relies on the interpolation, we first implement the interpolation method by extending the Image class  with the following method:\n\n\npublic float interpolatedAt(float x, float y)  \n\n\nThe method takes in a physical $(x,y)$ coordinate and returns the image intensity at that position.\nWe use bilinear interpolation to find the value at $(x,y)$ (described in the theory).\n\n\nWe can rewrite the interpolation equation using the linear interpolation formula when we want to interpolate between two points $x_1,x_2$ with function value $f(x_1),f(x_2)$ to find out the function value $f(x)$ at $x$.\n\n\n$$ \\frac{f(x) - f(x_1)}{x-x_1} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1} $$\n\n\n\n\nSince we already know the difference $x_2 - x_1$ is either 1.0 if we have a pixel spacing of 1.0 or pixel spacing, we can simplify the above equation as follows:\n\n$$f(x) = f(x_1) + (x-x_1) (f(x_2) - f(x_1))$$\n\n\n\nYou can use the function below to compute linear interpolation between two points $x_1,x_2$ at $x$\n\n // Definition of arguments\n // diff_x_x1 = x - x_1 compute the difference between point x and x_1\n // fx_1 = f(x_1), pixel value at point x_1\n // fx_2 = f(x_2), pixel value at point x_2 \n\n float linearInterpolation(float fx_1, float fx_2, float diff_x_x1) {\n     return fx_1 + diff_x_x1 * (fx_1 - fx_2);\n }\n \n\n\n\nWe now have an way to interpolate between two points in 1D. We need to extend this to 2D case such that we can use \nit for interpolating values in our image. An illustration of how this can be done is \nalready given in the theory section.\n\n\nImplementation detail We describe here possible way to implement the interpolation scheme.\n\n\nFind the 4 nearest pixel indices, for the given physical coordinate $(x,y)$. To do, this you have to transform\nthe physical coordinate to the index space of the image.\n\n\nHint: In physical space all the values of $x$ and $y$\nare computed from origin. So we just need to subtract the origin from the coordinates for this correction.\nx -= origin[0]\ny -= origin[1]\n\n\n\nPixel spacing also alters the physical coordinates and needs to be corrected for. \nThis can be done using just by dividing each coordinate by the pixel spacing.\nx /= spacing;\ny /= spacing\n\n\n\nHint: Since each pixel is a unit square you can round up and down each coordinate ($x$ and $y$) separately \nto get the 4 nearest pixels coordinates.\n\n\nInterpolate along an axis (here we choose the x-axis) initially using the linear interpolation \nfunction to obtain intermediate points.\n\n\nNow interpolate along the intermediate points (i.e you are interpolating along y-axis)\n\n\nNote: Take care of image origin and pixel spacing for the input coordinates before you perform any of the steps.\nAlso, always use atIndex and setIndex for accessing the image values. \nThis ensures that we handle the values at boundary correctly.\n\n\n\n\nExample:\nHere we look at a single point to understand how to implement our algorithm\n\n\nIf we have an input $(x,y) = (0.4,0.4)$, then the 4 nearest pixel coordinates are $(0,0)$,$(1,0),(1,1),(0,1)$\n\n\nInterpolating the values between the points $a = (0,0)$, $b = (1,0)$, find the intermediate \nvalue at point $I_1 = (0.4,0)$.\n\n\nSimilarly interpolate between $c = (0,1)$ and $d = (1,1)$ to find the intermediate value at point $I_2 = (0.4,1)$.\n\n\nNow we can just use the values at the intermediate points $I_1 = (0.4,0)$ and $I_2 = (0.4,1)$ and \nperform a linear interpolation in the y direction to obtain the final result at $(0.4,0.4)$.\n\n\n\n\n2. Image Transformation\n5 Points\nNow we can start with the implementation of ImageTransformer class.\n\nThe class consists of the following member functions for translation\n\n// Transformation parameters\npublic float shiftX; // tx\npublic float shiftY; // ty\npublic float rotation; // theta\npublic float scale; // s\n\n\n\nAlso use the interface ImageFilter abstract class which you have implemented in the previous exercises. \nThis can be done using implements keyword.\n\n\nAdd the method apply(Image input,Image output) which takes in two variables input and \noutput of Image class type. The input variable provides the input image to our transformer class. \nThe output variable is where the transformed image is stored.\n\n\nConsider each pixel in the image with index $(i,j)$. When we access an image pixel we get \nthe pixel intensity stored at the location $(i,j)$.\n\n\nHere $(i,j)$ represents the image coordinates $(x,y)$ and the pixel value at $(i,j)$ represents $f(x,y)$.\n\n\nWe want to transform $(x,y) \\to (x',y')$ and find the pixel value at the new location for a \ngiven set of input transformation parameters $t_x,t_y,\\theta,s$ to transform the input image coordinate $(x,y)$.\n\n\nLet us go over a possible approach to implement the apply method which \nimplements (translation,rotation and scaling). In addition, once we have the transformed coordinates $(x',y')$ we \ninterpolate the value at this coordinate to set the output value of the new image.\n\n\nWe can implement the transformations and interpolation using the equations defined \nin the theory section. \n\n\nHowever, from the implementation perspective it is much easier to ask what will be my output image value \nat the current position $(x',y')$  for the given  transformations parameters.\n\n\nFor this we need to find the input coordinate $(x,y)$ for the given transformation parameters.\nThis mapping from $(x',y') \\to (x,y)$ is known as the inverse transformation.\n\n\nJust to recap our current aim is to iterate over the output image along each \npixel $(i,j)$ (also referred as $(x',y')$) and find the inverse transformation (x,y).\nOnce we find $(x,y)$ we can just interpolate the values in the input image at $(x,y)$ and\nset it to the output image value at (x',y').\n\n\nAn example code to accomplish this looks like below:\n\n\n// We need to compute (x,y) from (x&#39;,y&#39;)\n// We use xPrime,yPrime in the code to indicate (x&#39;,y&#39;)\n// Interpolate the values at (x,y) from the input image to get\nfloat pixelValue = input.interpolatedAt(x,y);\n\n// Set your result at the current output pixel (x&#39;,y&#39;)\noutput.setAtIndex(xPrime, yPrime, pixelValue);\n\n\n\n\nThe inverse transformations can be computed using the following equations.\n\n\nTranslation\n\n$x =  x' - t_x$\n$y =  y' - t_y$ \n\n\n\nRotation\n\n$x= x' \\cos\\theta + y' \\sin\\theta$\n$y= - x \\sin\\theta + y' \\cos\\theta$\n\n\n\nScaling\n\n$x=  \\frac{x'}{s}$\n$y=  \\frac{y'}{s}$\n\n\n\nImplementation detail Now you can directly use the above equations to implement translation, rotation and scaling.\nThe entire apply method for the ImageTransformer class can be implemented as follows:\n\n\nIterate over each pixel in the output image (although they are just the same as input initially).\n\n\nAt each pixel the index $(i,j)$ represents our coordinates $(x',y')$ of the output image\n\n\nApply the transformations using the equations described above to find $(x,y)$\n\n\nNow set the output image value at $(i,j)$ (also referred as (x',y')) from the interpolated values at $(x,y)$ \nfrom the input image.\n\n\nUse the setIndex() for setting the values of the output image and atIndex() for getting the values \nfrom input image.\n\n\nIn the above formulation we assume that we have pixel spacing of $spacing = 1.0$ and the \nimage origin at $(x_0, y_0) = (0,0)$.\n\n\nYou can extend this to work for different values of pixel spacing and origin.\n\n\nHint: Think of pixel spacing as a scaling and origin as a translation transformation. \n(apply both spacing and origin transformation to the input coordinates $(x,y)$ as  $(x * px , y * py) + (x_0,y_0))$ \n\n\n\n\n"
},

{
"title": "Exercise 3",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-3/",
"body": "Submission deadline: 25.05.20 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImages and 2-d Convolution\nIn this exercise, we finally to work with images. It's time to update the file src/main/java/lme/DisplayUtils.java to the newest version.\nThis should provide you the following methods to work with images:\n    // Open a file\n    public static mt.Image openImage(String path) \n\n    // Download and open a file from the internet\n    public static mt.Image openImageFromInternet(String url, String filetype) \n\n    // Save an image to a file\n    public static void saveImage(mt.Image image, String path) \n\n    //  Show images\n    public static void showImage(float[] buffer, String title, int width) \n    public static void showImage(float[] buffer, String title, long width, float[] origin, double spacing, boolean replaceWindowWithSameName)\n\n\nThey all work with the class mt.Image so let's create it!\nBefore that, add the following two methods to your Signal class (they are used by the tests of this exercise):\n    // Needs: import java.util.Random\n    public void addNoise(float mean, float standardDeviation) {\n\tRandom rand = new Random();\n\tfor (int i = 0; i &lt; buffer.length; i++) {\n\t    buffer[i] += mean + rand.nextGaussian() * standardDeviation;\n\t}\n    }\n\n    public void setBuffer(float[] buffer) {\n\tthis.buffer = buffer;\n    }\n\nPS: The method addNoise is also useful to test your mean and standardDeviation calculation in exercise 2.\nCreate a long signal and add noise with a specific mean and standardDeviation.\nThe result of your mean and standardDeviation method should be approximatelly the same.\nmt/Image.java\n4 Points\nThe code for this section should go to src/main/java/mt/Image.java\nOur goal is to share as much code with our mt.Signal class. So mt.Image will be a subclass of mt.Signal.\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\nimport lme.DisplayUtils;\n\npublic class Image extends Signal {\n\n\n}\n\nmt.Image has five members (apart from the ones inherited by mt.Signal).\n    // Dimensions of the image\n    protected int width; \n    protected int height; \n\n    // Same as Signal.minIndex but for X and Y dimension\n    protected int minIndexX;\n    protected int minIndexY;\n\n    // For exercise 4 (no need to do anything with it in exercise 3)\n    protected float[] origin = new float[]{ 0, 0 };\n\nAnd two constructors:\n    // Create an image with given dimensions\n    public Image(int width, int height, String name)\n\n    // Create an image with given dimensions and also provide the content\n    public Image(int width, int height, String name, float[] pixels)\n\nAs shown in the exercise slides, we will store all the pixels in one array, like we did in Signal.\nThe array should have the size width * height.\nminIndexX,minIndexY should be 0 for normal images.\n\n\nCall the constructors of the super class Signal in the constructors of Image.\nYou can call the constructor of a super class by placing super(...) with the respetive arguments in the first line of the constructor of the subclass.\nThe constructor public Image(int width, int height, String name, float[] pixels) does not need to create its own array (take pixels for buffer).\nBut you can check whether pixels has the correct size.\nLet's also provide some getters!\n    // Image dimensions\n    public int width()\n    public int height()\n\n    // Minimum and maximum indices (should work like Signal.minIndex/maxIndex)\n    public int minIndexX()\n    public int minIndexY()\n    public int maxIndexX()\n    public int maxIndexY()\n\natIndex and setAtIndex should work like in Signal except that they now have two coordinate indices.\natIndex should return 0.0f if either the x or y index are outside of the image ranges.\n    public float atIndex(int x, int y)\n    public void setAtIndex(int x, int y, float value) {\n\nRemember how we calculated the indices in the exercise slides. You have to apply that formula in atIndex/setAtIndex.\n\n\nAdd the method show to display the image\n    public void show() {\n        DisplayUtils.showImage(buffer, name, width(), origin, spacing(), /*Replace window with same name*/true);\n    }\n\nOpen the image pacemaker.png in a file  src/main/java/exercise/Exercise03 (in the same project as previous exercise):\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage exercises;\n\nimport mt.GaussFilter2d;\nimport mt.Image;\n\npublic class Exercise03 {\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n        Image image = lme.DisplayUtils.openImageFromInternet(&quot;https://mt2-erlangen.github.io/pacemaker.png&quot;, &quot;.png&quot;);\n        image.show();\n\n    }\n}\n\nThe image is from our open access book.\n\nmt.ImageFilter\n3 Points:\nLike in Exercise 1, we want to be able to convolve our image signal.\nInfact, we will learn a lot of new ways to process images.\nOften, we need to create an output image of same size.\nLet's create an interface (src/main/java/mt/ImageFilter.java) for that, so we only need to implement this once.\npackage mt;\n\npublic interface ImageFilter {\n    default mt.Image apply(mt.Image image) {\n        Image output = new Image(image.width(), image.height(), image.name() + &quot; processed with &quot; + this.name());\n        apply(image, output);\n        return output;\n    }\n\n    default void apply(mt.Image input, mt.Image output) {\n        throw new RuntimeException(&quot;Please implement this method!&quot;);\n    }\n\n    String name();\n}\n\nThe code for the convolution should go to src/main/java/mt/LinearImageFilter.java\nOk. Now the convolution. The class has already a method that we will need later. It uses your sum method.\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class LinearImageFilter extends Image implements ImageFilter {\n\n    public void normalize() {\n\tdouble sum = sum();\n\tfor (int i = 0; i &lt; buffer.length; i++) {\n\t    buffer[i] /= sum;\n\t}\n    }\n}\n\nCreate a constructor for it. Recall how we implemented LinearFilter!\nminIndexX and minIndexY need to be set to $-\\lfloor L_x/2 \\rfloor$ and $-\\lfloor L_y/2 \\rfloor$ when $L_x$ is the\nfilter's width and $L_y$ the filter's height.\n    public LinearImageFilter(int width, int height, String name)\n\nConvolution in 2-d works similar to convolution in 1-d.\n$$K_x = \\lfloor L_x/2 \\rfloor$$\n$$K_y = \\lfloor L_y/2 \\rfloor$$\n$$g[x,y] = \\sum_{y'=-K_y}^{+K_y} \\sum_{x'=-K_x}^{+K_x} f[x-x', y-y'] \\cdot h[ x', y' ] $$\n$$g[x,y] = \\sum_{y'=\\text{h.minIndexY}}^{\\text{h.maxIndexY}} \\sum_{x'=\\text{h.minIndexX}}^{\\text{h.maxIndexX}} f[x-x', y-y'] \\cdot h[ x', y' ] $$\nRemember to use atIndex and setAtIndex to get and set the values.\nImplement the convolution in the method apply.\nThe result image was already created by our interface ImageFilter.\n    public void apply(Image image, Image result)\n\n\n\nSource: https://github.com/vdumoulin/conv_arithmetic\nNow it's time to test!\nUse the file src/test/java/mt/LinearImageFilterTests.java.\nGauss Filter\n2 Points\nThe code for the Gauss filter should go to src/main/java/mt/GaussFilter2d.java.\nThe Gauss filter is a LinearImageFilter with special coefficients (the filter has the same height and width).\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class GaussFilter2d extends LinearImageFilter {\n    \n}\n\nIt has the following constructor\n    public GaussFilter2d(int filterSize, float sigma)\n\nIn the constructor, set the coefficients according to the unormalized 2-d normal distribution with standard deviation $\\sigma$ (sigma).\nMath.exp is the exponetial function.  Use setAtIndex: $x$ should run from minIndexX to maxIndexX and $y$ from minIndexY to maxIndexY.\n$$ h[x,y] = \\mathrm{e}^{-\\frac{x^2+y^2}{2 \\sigma^2}}$$\nCall normalize() at the end of the constructor to ensure that all coefficients sum up to one.\nTest your Gauss filter in Exercise03.java.\nUse arbitray values for sigma and filterSize.\nThe Gauss filter will blur your input image clearly if you chose a large value for sigma.\n\nThere is also a unit test file that you can use: src/test/java/mt/GaussFilter2dTests.java\nCalculating with Images\n1 Points\nThe code for this section should go to src/main/java/mt/Image.java.\nImplement the method Image.minus in Image.java that subtracts the current image element-wise with another one and returns the result:\n    public Image minus(Image other)\n\nWe use this method to calculate error images.\nYou can implement this with only one loop over the elements of the buffers of the two images.\nDemo\nThis is not required for the exercise!\nPlace the file src/main/java/exercises/Exercise03Demo.java\nin your project folder and run it.\n\nYou should see an interactive demo applying your Gauss filter to a noisy image.\nYou change change the used parameters.\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it on studOn.\n\n\n\n\n\n"
},

{
"title": "Exercise 2",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-2/",
"body": "\n\n\n\n\nSubmission deadline: 18.05.20 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nStatistical Measures\nIn this exercise, we want to have a look on how we can analyze signals using simple statistical measures.\nWe will use a freely available ECG data set with the goal to distinguish healthy from patients with heart rythm problems.\n\nYou can find the original data set here\nbut we recommend to use a post-processed version available on studOn.\nGradle Build System\nIn Medizintechnik II we use the build system Gradle.\nGradle is especially popular for Android projects since it's easy to add new software dependencies that will be automatically\ndownloaded.\nIn our case, the published data set is saved as Matlab *.mat files.\nTo read those files, an external dependency was already added to our build.gradle file.\n    implementation &#39;us.hebi.matlab.mat:mfl-core:0.5.6&#39;\n\ndoes the magic and automatically downloaded a *.mat file reader.\nIn case, you need to add external software to your own projects you can use this search engine.\nTasks\nLoading one of File of the Data Set\nLoad the file src/main/java/exercises/Exercise02.java (available here (Click the raw button)) into your existing project.\nIt alread contain some code for parsing the program parameters:\n    public static void main(String[] args) throws IOException {\n\t(new ij.ImageJ()).exitWhenQuitting(true);\n\n\tSystem.out.println(&quot;Started with the following arguments:&quot;);\n\tfor (String arg : args) {\n\t    System.out.println(arg);\n\t}\n\n\tif (args.length == 1) {\n\t    File file = new File(args[0]);\n\t    if (file.isFile()) {\n\t\t// Your code here:\n\n\n\t    } else {\n\t\t    System.err.println(&quot;Could not find &quot; + file);\n\t    }\n\n\t} else {\n\t    System.out.println(&quot;Wrong argcount: &quot; + args.length);\n\t    System.exit(-1);\n\t}\n\nLaunch Exercise02 with the one of the files of the data set as an argument (e.g. &lt;where_you_saved_your_data_set&gt;/MLII/1 NSR/100m (0).mat)!\n\nHow to do that in Eclipse\nHow to do that in IntelliJ\n\nYour program should print now the file name you selected:\n\nRemember to never put file names directly in your code. Your program will then only work on your machine!\nLet's open this file!\nif (file.isFile()) {\n    // A file should be opened \n    us.hebi.matlab.mat.types.Matrix mat = Mat5.readFromFile(file).getMatrix(0);\n    Signal heartSignal = new mt.Signal(mat.getNumElements(), &quot;Heart Signal&quot;);\n    for (int i = 0; i &lt; heartSignal.size(); ++i) {\n\t    heartSignal.buffer()[i] = mat.getFloat(i);\n    }\n    heartSignal.show();\n\n\n} else if (file.isDirectory()) {\n\nYou should now see the signal. However, this plot does not have any labels with physical units attached.\nWe will change that later.\n\nExtension of Signal.java\n4 Points\nTo analyze this and other signals, we will extend our Signal class.\nPlease implement the following methods in Signal.java that calculate some descriptive properties of the signal:\n    public float min()        //&lt; lowest signal value\n    public float max()        //&lt; largest signal value\n    public float sum()        //&lt; sum of all signal values\n    public float mean()       //&lt; mean value of the signal\n    public float variance()   //&lt; variance of the signal\n    public float stddev()     //&lt; standard deviation of the signal\n\nTest the methods in your main function and check whether the calculated values seem plausible\nby looking at your plot and printing the calculated values.\nPhysical Dimensions\n1 Points\nThe code for this section belong to Signal.java\nIn the last exercise, we treated signals as pure sequence of numbers without any physical dimensions.\nBut for medical measurements physical dimensions are important.\nWe want to extend our plot to look like this with the horizontal axis labeled with seconds:\n\nTo do this we will add a new member to our signal that's describing the physical distance between two samples\n    protected float spacing = 1.0f;  //&lt; Use 1.0f as a default when we don&#39;t set the physical distance between points\n\nAdd also a setter and getter method\n    public void setSpacing(float spacing) \n    public float spacing() \n\nRead in the discription of the data set the sampling frequency of the signal\nand use it to calculate the spacing between two samples. Set this property setSpacing in the main method.\nNext, we want to change show() to regard our spacing and to accept a ij.gui.Plot so that we can set the axis of our plot.\n    public void show(Plot plot) {\n\t    DisplayUtils.showArray(buffer, plot, /*start of the signal=*/0.f, spacing);\n    }\n\nBecause we are lazy, we can still keep the original usage of show()\n    public void show() {\n\t    DisplayUtils.showArray(buffer, name, , /*start of the signal=*/0.f, spacing);\n    }\n\nPlease create an instance of ij.gui.Plot in the main method of Exercise02 with descriptive labels for both axis and use if for heartSignal.show(...).\n\n// Constructs a new Plot with the default options.\nPlot plot = new Plot(&quot;chosee title here&quot;, &quot;choose xLabel here&quot;, &quot;choose yLabel here&quot;)\nheartSignal.show(plot);\n\n//... add here more plotting stuff\n\nplot.show()\n\nDetermine the Heart Frequency\n5 Points\nThe remainder of this exercise will be implemented in Exercise02.java\nCreate a file  src/main/java/lme/HeartSignalPeaks.java with following content\npackage lme;\n\nimport java.util.ArrayList;\n\npublic class HeartSignalPeaks {\n\tpublic ArrayList&lt;Double&gt; xValues = new ArrayList&lt;Double&gt;();\n\tpublic ArrayList&lt;Double&gt; yValues = new ArrayList&lt;Double&gt;();\n}\n\nArrayList behave like arrays, except you can add new items to make it longer. You can read more about them here.\nWe now want to find the peaks of the heart signal. We do that by finding local maxima within region that are above a certain\nthreshold (here in blue).\nFind a good value of this threshold so that all peaks are above this value.\nYou may use mean(), max(), min() to calculate it.\nYou can see your threshold by ploting it:\n    plot.setColor(&quot;blue&quot;);\n    plot.add(&quot;lines&quot;, new double[] { 0, /* a high value */10000 }, new double[] { threshold, threshold });\n\n\nImplement the following method that finds all peaks of the signal.\n    public static lme.HeartSignalPeaks getPeakPositions(mt.Signal signal, float threshold)\n\nTo determine the signal peaks, one can use normal maximum search over the signal values.\nSave the found maximum value (i.e. signal amplitude) in x(max) and\nthe location of maximum (i.e. the time at which the peak occurs) in y(arg max).\nYou can implement the peak finding method as follows:\n\n\nLoop over the signal and at each index\n\n\nUse  boolean variable to determine if the current signal value is above the threshold.\n\n\nIf the previous signal value was above the threshold (i.e boolean value was true), and the current value is below threshold (i.e boolean value is false)\n\n\nAdd the previous signal value as a instance of HeartSignalPeaks (like peaks.xValues and peaks.yValues)\n\n\nThis is a suggested workflow, but feel free to use your own ideas to efficently find the peaks of the signal.\nYou can plot the peaks you have found:\n    plot.setColor(&quot;red&quot;);\n    plot.addPoints(peaks.xValues, peaks.yValues, 0);\n\nNext, create a Signal with the difference in time between succesive peaks (import import java.util.ArrayList;). \n\tpublic static mt.Signal calcPeakIntervals(lme.HeartSignalPeaks peaks) {\n\t\tArrayList&lt;Double&gt; peakPositions = peaks.xValues;\n\t\tif  (peakPositions.size() &gt; 1) {\n\t\t\tSignal intervals = new mt.Signal(peaks.xValues.size() - 1, &quot;Peak Intervals&quot;);\n\n\t\t\tfor (int i = 0; i &lt; peakPositions.size() - 1; ++i) {\n\t\t\t\tintervals.buffer()[i] = (float) (peakPositions.get(i + 1) - peakPositions.get(i));\n\t\t\t}\n\t\t\treturn intervals;\n\t\t} else {\n\t\t\treturn new mt.Signal(1, &quot;No Intervals found&quot;);\n\t\t}\n\t}\n\nYou can use that signal to determine the mean cycle duration (peakIntervals.mean()), the mean heart frequency ((1. / intervals.mean())) and\nbeats per minute (60. * 1. / intervals.mean()). Print those values!\nSummary of tasks\nTo summarize the list of tasks that needs to be implemented to complete this exercise\n\nSet the file path correctly to load the signal into your program (Ensures you can load the signal inside the program)\nAdd labels to the plot, include spacing variable in signal class for visualizing plots in physical dimensions.\nImplement methods to compute statistical measures (like mean, median,...). (Use the formula provied in lecture/exercise slides)\nDetermine the threshold (follow the description provided here)\nFind the peaks (follow the description provided here)\nCalculate intervals between the peaks\n\nNote\nWhile setting file path as arguments, add &quot;path&quot; if there are spaces in file name since java parses space as new arguments.\nBonus\nThis is not required for the exercise.\nRun Exercise02 with other files of the data set as an argument.\nWhat is the meaning of the mean value and the variance of time distance between the peeks?\nHow do signals with low variance in the peak distances look like and how signals with high variance?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
},

{
"title": "Exercise 1",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-1/",
"body": "Signals and Convolution\nSubmission deadline: 11.05.20 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImageJ\nThe image processing program we want to use during this semester is called ImageJ.\nIt was developed at the US National Institutes of Health and is used nowadays especially in research \nfor medical and biological images.\nIf you want to, you can download a stand-alone version of the program here.\nGetting started\nImageJ can also be used as a Java library.\nWe already created a Java project that uses ImageJ.\nYou can download it from https://github.com/mt2-erlangen/exercises-ss2021 and import with the IDE of your choice:\n\nInstructions for Eclipse\nInstructions for IntelliJ\n\nTasks\n\n\nYou should now be able to execute the file src/main/java/exercises/Exercise01.java\n\n\nThe following code is opening the ImageJ main window and exits the running program when the window is closed.\npublic class Exercise01 {\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n    }\n}\n\nIntelliJ will only allow to run Exercise01 when there are no errors in the project. You can just out-comment the method lme.Algorithms.convolution1d until you implemented your Signal class.\nSignal.java\n4 Points\nAs a first step, we will implement the class Signal \nwhich should hold a signal of finite length.\nCreate the file src/main/java/mt/Signal.java.\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\nimport lme.DisplayUtils;\nimport ij.gui.Plot;\n\npublic class Signal {\n\n}\n\nSignal should have the following members\n    protected float[] buffer; // Array to store signal values\n    protected String name;    // Name of the signal\n    protected int minIndex;   // Index of first array element (should be 0 for signals)\n\nImplement two constructors for Signal\n    public Signal(int length, String name)     // Create signal with a certain length (set values later)\n    public Signal(float[] buffer, String name) // Create a signal from a provided array\n\nImplement the following getter methods for Signal\n    public int size()        // Size of the signal\n    public float[] buffer()  // Get the internal array \n    public int minIndex()    // Get lowest index of signal (that is stored in buffer)\n    public int maxIndex()    // Get highest index of signal (that is stored in buffer)\n    public String name()     // Get the name of the signal\n\nNext, we want to visualize our Signal in the method show. You can use provided function lme.DisplayUtils.showArray.\nTo test it, create a Signal with arbitray values in the main method of Exercise01 and call its show method.\n    public void show() {\n        DisplayUtils.showArray(this.buffer, this.name, /*start index=*/0, /*distance between values=*/1);\n    }\n\nIn our black board exercises, we agreed that we want to continue our signals with zeros where we don't have any values stored.\nIf we access indices of our Signal with values smaller than minIndex() or larger maxIndex() we want to return 0.0f.\nIf a user accesses an index between minIndex() and maxIndex() we want to return the corresponding value stored in our array.\n\nImplement the method atIndex and setAtIndex. Please be aware that minIndex can be smaller than 0 for subclasses of Signal.\nIf setAtIndex is called with an invalid index (smaller than minIndex or greater than maxIndex), it's ok for the program to crash.\nThis should not happen for atIndex.\n    public float atIndex(int i)\n    public void setAtIndex(int i, float value)\n\nYou can check the correctness of atIndex/setAtIndex with the test testAtIndex in file src/test/java/SignalTests.java.\nLinearFilter.java\n3 Points\nImplement LinearFilter in file src/main/java/LinearFilter.java as a subclass of Signal.\nLinearFilter should work like Signal except its minIndex should be at - floor(coefficients.length/2) as in the exercise slides.\n\nLinearFilter should have a constructor that checks that coefficients is an array of odd size or throws an error otherwise (any error is ok).\n    public LinearFilter(float[] coefficients, String name)\n\nand a method that executes the discrete convolution on another Signal input and returns an output of same size.\n   public Signal apply(Signal input);\n\nYou should be able to directly use the formula from the exercise slides (f is the input signal, h our filter, $L$ the filter length)\n$$K = \\lfloor L/2 \\rfloor$$\n$$g[k] = \\sum_{\\kappa=-K}^{K} f[k-\\kappa] \\cdot h[ \\kappa ]$$\nor with our minIndex/maxIndex methods for each index $k$ of the output signal.\n$$g[k] = \\sum_{\\kappa=h.\\text{minIndex}}^{h.\\text{maxIndex}} f[k-\\kappa] \\cdot h[\\kappa] $$\nBe sure that you use atIndex to access the values of input and the filter.\n\nYou can test your convolution function with the tests provided in src/test/java/LinearFilterTests.java.\nGood test cases are:\n\n{0,0,1,0,0}: this filter should not change your signal at all\n{0,1,0,0,0}: this filter should move your signal one value to the left\n{0,0,0,1,0}: this filter should move your signal one value to the right\n\nQuestions\n3 Points\nIn this task we want to convolve a test Signal with three different linear filters.\nFilter the signal $f[k]$  Signal(new float[]{0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0}, &quot;f(k)&quot;)\nwith filters\n\n$h_1[k]$: {1.0f/3 ,1/3.f ,1/3.f},\n$h_2[k]$: {1/5.f, 1/5.f , 1/5.f, 1/5.f, 1/5.f},\n$h_3[k]$: {0.5f, 0, -0.5f}.\n\nSave the images of the input signal and filtered results (recommended filetype: png).\nCreate a PDF document (e.g. with Word or LibreOffice) with those images in which you describe briefly how the filters modified the input signal and why.\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it and your PDF document via StudOn!\n"
}
]
